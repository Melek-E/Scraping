# -*- coding: utf-8 -*-
"""FlightScraper python bot for kayak.ipynb

Automatically generated by Colab.
"""

from time import sleep, strftime
from random import randint
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import os

def parse_raw_text(raw_text):
    """Parse raw flight data text into a structured list of dictionaries."""
    # Split the text into sections based on "Save", "Share", or "Ad" keywords
    sections = re.split(r'\bSave\b|\bShare\b|\bAd\b', raw_text)

    flights = []
    for section in sections:
        # Clean up the section
        section = section.strip()
        if not section:
            continue

        # Extract data using regex
        match = re.search(
            r'(?P<date>\d{1,2}/\d{1,2})\n(?P<day>\w+)\n(?P<time>[\d:apm â€“]+)\n(?P<airline>[\w\s]+)\n'
            r'(?P<stops>\w+)\n(?P<duration>\d+h \d+m)\n(?P<from>\w+)\n-\n(?P<to>\w+)',
            section,
        )
        if match:
            flight = match.groupdict()

            # Optional: Extract price if available
            price_match = re.search(r'\$\d+', section)
            flight['price'] = price_match.group() if price_match else 'N/A'

            flights.append(flight)

    return flights

driver = webdriver.Chrome()
driver.maximize_window()

sleep(2)



# Function to load more results
def load_more():
    try:
        # Wait and click the "Show more results" button
        show_more_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH,
                                        "//div[@role='button' and contains(@class, 'show-more-button') and text()='Show more results']"))
        )
        show_more_button.click()
    except:
        # Break if the button is no longer present or clickable
        print("MISTAKEEEEE")
        pass



# Start scraping from Kayak
def start_kayak(city_from, city_to, date_start, date_end):
    kayak = ('https://www.kayak.com/flights/' + city_from + '-' + city_to +
             '/' + date_start + '-flexible/' + date_end + '-flexible?sort=bestflight_a')
    driver.get(kayak)

    for i in range(3):
        sleep(randint(5))
        load_more()

    try:
        # Wait until the page is fully loaded before proceeding
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//a[@class = "moreButton"]')))
        sleep(randint(8, 10))

        xp_popup_close = WebDriverWait(driver, 15).until(EC.element_to_be_clickable(
            (By.XPATH,
             '//button[contains(@id,"dialog-close") and contains(@class,"Button-No-Standard-Style close ")]')))[5]
        xp_popup_close.click()
    except Exception as e:
        print("No popup appeared or other error:", e)

    sleep(randint(20, 39))
    print('loading more.....')

    # First scrape
    print('starting first scrape.....')
    df_flights_best = page_scrape()
    df_flights_best['sort'] = 'best'
    sleep(randint(10, 30))

    # Ensure the matrix prices section is fully loaded
    matrix = WebDriverWait(driver, 15).until(
        EC.presence_of_all_elements_located((By.XPATH, '//*[contains(@id,"FlexMatrixCell")]')))
    matrix = driver.find_elements(By.XPATH,'//*[contains(@id,"FlexMatrixCell")]')
    print("matrix", matrix)
    matrix_prices = [price.text.replace('$', '') for price in matrix]
    matrix_prices = [int(price) for price in matrix_prices if price != '']

    print('matrix prices', matrix_prices)

    matrix_prices = list(map(int, matrix_prices))
    matrix_min = min(matrix_prices)
    matrix_avg = sum(matrix_prices) / len(matrix_prices)
    print(matrix)
    print('switching to cheapest results.....')
    cheap_results = WebDriverWait(driver, 25).until(EC.element_to_be_clickable((By.XPATH, '//div[@data-content = "price_a"]')))

    try:
        cheap_results.click()
    except:
        pass
    print("switched to cheapest results.....")
    sleep(randint(20, 50))
    print('loading more.....')

    # Second scrape
    print('starting second scrape.....')
    df_flights_cheap = page_scrape()
    df_flights_cheap['sort'] = 'cheap'
    sleep(randint(20, 45))

    print('switching to quickest results.....')
    '''for i in range(3):
        sleep(randint(5))
        load_more()'''
    quick_results = WebDriverWait(driver, 15).until(
        EC.element_to_be_clickable((By.XPATH, '//div[@data-content = "duration_a"]')))
    quick_results.click()
    print("switched to quickest")

    sleep(randint(20, 50))
    print('loading more.....')

    # Third scrape
    print('starting third scrape.....')
    df_flights_fast = page_scrape()
    df_flights_fast['sort'] = 'fast'
    sleep(randint(30, 70))

    # Ensure the 'search_backups' directory exists
    if not os.path.exists('search_backups'):
        os.makedirs('search_backups')

    # Saving the final DataFrame as an Excel file
    final_df = pd.concat([df_flights_best, df_flights_cheap, df_flights_fast])
    final_df.to_excel(os.path.join('search_backups', '{}_flights_{}-{}_from_{}_to_{}.xlsx'.format(
        strftime("%Y%m%d-%H%M"), city_from, city_to, date_start, date_end)), index=False)

    print('Excel file saved successfully in search_backups folder.')

    print('saved df.....')

    ''''# We can keep track of what they predict and how it actually turns out!
    xp_loading = driver.find_element(By.XPATH, '//div[contains(@id,"advice")]').text
    xp_prediction = driver.find_element(By.XPATH, '//span[@class="info-text"]').text
    print(xp_loading + '\n' + xp_prediction)'''

    ''''# sometimes we get this string in the loading variable, which will conflict with the email we send later
    weird = '\u00af\\_(\u30c4)_/\u00af'
    if xp_loading == weird:
        xp_loading = 'Not sure'''''

# The page scraping function
def page_scrape():
    """This function takes care of the scraping part"""

    xp_sections = driver.find_elements(By.XPATH, '//*[@class="Fxw9"]')
    print('xp_sections', xp_sections)
    sections_list = [value.text for value in xp_sections]
    print("sections_list", sections_list)

    # Parse the raw text using parse_raw_text
    parsed_flights = parse_raw_text("\n".join(sections_list))
    print(parsed_flights)
    # Convert parsed data to a DataFrame
    flights_df = pd.DataFrame(parsed_flights)
    flights_df['timestamp'] = strftime("%Y%m%d-%H%M")  # So we can know when it was scraped

    return flights_df

# Input details
city_from = 'TUN' #input('From which city? ')
city_to =  'PAR' #input('Where to? ')
date_start = '2025-01-10' #input('Search around which departure date? Please use YYYY-MM-DD format only ')
date_end = '2025-01-20' #input('Return when? Please use YYYY-MM-DD format only ')


#if we want to check different prices day to day, or hour to hour we can change the range and sleep amount
#keka najmou ncompariw 3ala periode atwel, but for convenience sake
#imma leave it at range(1) and no sleep
for n in range(1):
    start_kayak(city_from, city_to, date_start, date_end)
    print('iteration {} was complete @ {}'.format(n, strftime("%Y%m%d-%H%M")))


    sleep(1)
    print('sleep finished.....')

# Bonus: save a screenshot!
driver.save_screenshot('pythonscraping.png')

driver.quit()
